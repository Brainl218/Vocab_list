{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "#!pip install pandas\n",
    "#!pip install selenium\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "DRIVER_PATH = './chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "base_url = \"https://www.oxfordlearnersdictionaries.com/topic/\"\n",
    "BASE_PATH = \"./oxford-vocab/\"\n",
    "\n",
    "if not os.path.isdir(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(base_url)\n",
    "\n",
    "if r.status_code == requests.codes.ok:\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "topic_links = []\n",
    "topics = soup.find_all(\"div\", class_=\"topic-box\")\n",
    "for topic in topics:\n",
    "    link = topic.find(\"a\").get(\"href\")\n",
    "    topic_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_scraping(url, PATH):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    word_list = []\n",
    "    word_list_box = driver.find_element_by_xpath('.//ul[@class=\"top-g\"]')\n",
    "    word_list_box_elements = word_list_box.find_elements_by_xpath('.//li')\n",
    "    \n",
    "    for ele in word_list_box_elements:\n",
    "        if ele.get_attribute(\"class\") != \"hidden\": \n",
    "            vocab = ele.find_element_by_xpath('.//a').text\n",
    "            pos = ele.find_element_by_xpath('.//span[@class=\"pos\"]').text\n",
    "            level = ele.find_element_by_xpath('.//span[@class=\"belong-to\"]').text\n",
    "            word_list.append((vocab,pos,level))\n",
    "    print(f\"Found {len(word_list)} vocabs.\")\n",
    "    \n",
    "    word_list = sorted(word_list, key=lambda x: x[2])\n",
    "    df = pd.DataFrame(word_list, columns=[\"vocab\", \"pos\" ,\"CEFR\"])\n",
    "    df.to_csv(PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: animals_1:   0%|          | 0/18 [00:00<?, ?it/s]Found 530 vocabs.\n",
      "Found 55 vocabs.\n",
      "Found 10 vocabs.\n",
      "Found 17 vocabs.\n",
      "Found 68 vocabs.\n",
      "Found 51 vocabs.\n",
      "Found 19 vocabs.\n",
      "Found 52 vocabs.\n",
      "Found 37 vocabs.\n",
      "Found 58 vocabs.\n",
      "Found 154 vocabs.\n",
      "Found 9 vocabs.\n",
      "Found 264 vocabs.\n",
      "Found 37 vocabs.\n",
      "Found 6 vocabs.\n",
      "Found 203 vocabs.\n",
      "Found 18 vocabs.\n",
      "Found 147 vocabs.\n",
      "Found 98 vocabs.\n",
      "Found 11 vocabs.\n",
      "Found 11 vocabs.\n",
      "Found 27 vocabs.\n",
      "Found 119 vocabs.\n",
      "Found 119 vocabs.\n",
      "Found 11 vocabs.\n",
      "Found 4 vocabs.\n",
      "Found 63 vocabs.\n",
      "Found 12 vocabs.\n",
      "Found 8 vocabs.\n",
      "Found 10 vocabs.\n",
      "Found 11 vocabs.\n",
      "processing: appearance_1:   6%|▌         | 1/18 [04:57<1:24:25, 297.98s/it]Found 538 vocabs.\n",
      "Found 110 vocabs.\n",
      "Found 47 vocabs.\n",
      "Found 54 vocabs.\n",
      "Found 135 vocabs.\n",
      "Found 37 vocabs.\n",
      "Found 13 vocabs.\n",
      "Found 29 vocabs.\n",
      "Found 39 vocabs.\n",
      "Found 74 vocabs.\n",
      "Found 299 vocabs.\n",
      "Found 40 vocabs.\n",
      "Found 29 vocabs.\n",
      "Found 84 vocabs.\n",
      "Found 29 vocabs.\n",
      "Found 63 vocabs.\n",
      "Found 54 vocabs.\n",
      "Found 566 vocabs.\n",
      "Found 32 vocabs.\n",
      "Found 128 vocabs.\n",
      "Found 62 vocabs.\n",
      "Found 56 vocabs.\n",
      "Found 146 vocabs.\n",
      "Found 54 vocabs.\n",
      "Found 30 vocabs.\n",
      "Found 50 vocabs.\n",
      "Found 8 vocabs.\n",
      "Found 356 vocabs.\n",
      "Found 250 vocabs.\n",
      "Found 106 vocabs.\n",
      "processing: communication:  11%|█         | 2/18 [12:00<1:29:27, 335.48s/it]Found 477 vocabs.\n",
      "Found 86 vocabs.\n",
      "Found 169 vocabs.\n",
      "Found 35 vocabs.\n",
      "Found 92 vocabs.\n",
      "Found 66 vocabs.\n",
      "Found 29 vocabs.\n",
      "Found 357 vocabs.\n",
      "Found 50 vocabs.\n",
      "Found 31 vocabs.\n",
      "Found 31 vocabs.\n",
      "Found 51 vocabs.\n",
      "Found 63 vocabs.\n",
      "Found 34 vocabs.\n",
      "Found 63 vocabs.\n",
      "Found 34 vocabs.\n",
      "processing: culture:  17%|█▋        | 3/18 [15:23<1:13:54, 295.61s/it]      Found 247 vocabs.\n",
      "Found 25 vocabs.\n",
      "Found 99 vocabs.\n",
      "Found 47 vocabs.\n",
      "Found 39 vocabs.\n",
      "Found 37 vocabs.\n",
      "Found 433 vocabs.\n",
      "Found 51 vocabs.\n",
      "Found 18 vocabs.\n",
      "Found 14 vocabs.\n",
      "Found 51 vocabs.\n",
      "Found 57 vocabs.\n",
      "Found 78 vocabs.\n",
      "Found 46 vocabs.\n",
      "Found 30 vocabs.\n",
      "Found 57 vocabs.\n",
      "Found 31 vocabs.\n",
      "Found 345 vocabs.\n",
      "Found 15 vocabs.\n",
      "Found 57 vocabs.\n",
      "Found 28 vocabs.\n",
      "Found 32 vocabs.\n",
      "Found 46 vocabs.\n",
      "Found 39 vocabs.\n",
      "Found 28 vocabs.\n",
      "Found 54 vocabs.\n",
      "Found 46 vocabs.\n",
      "Found 579 vocabs.\n",
      "Found 52 vocabs.\n",
      "Found 41 vocabs.\n",
      "Found 31 vocabs.\n",
      "Found 124 vocabs.\n",
      "Found 65 vocabs.\n",
      "Found 106 vocabs.\n",
      "Found 39 vocabs.\n",
      "Found 38 vocabs.\n",
      "Found 83 vocabs.\n",
      "Found 426 vocabs.\n",
      "Found 105 vocabs.\n",
      "Found 20 vocabs.\n",
      "Found 71 vocabs.\n",
      "Found 59 vocabs.\n",
      "Found 60 vocabs.\n",
      "Found 48 vocabs.\n",
      "Found 63 vocabs.\n",
      "processing: culture:  17%|█▋        | 3/18 [26:34<2:12:51, 531.46s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './oxford-vocab/Culture\\\\TV, radio and news\\\\Watching TV/Listening to the radio.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-27cecc8e8615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink_path_temp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mvocab_scraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-311181c5a87f>\u001b[0m in \u001b[0;36mvocab_scraping\u001b[1;34m(url, PATH)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pos\"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m\"CEFR\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './oxford-vocab/Culture\\\\TV, radio and news\\\\Watching TV/Listening to the radio.csv'"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(topic_links), file=sys.stdout) as pbar:\n",
    "    for link_ in topic_links:\n",
    "        pbar.set_description('processing: ' + link_[link_.find(\"category/\")+len(\"category/\"):])\n",
    "        driver.get(link_)\n",
    "\n",
    "        topic = driver.find_element_by_xpath('//h1[@class=\"topic_label\"]').text\n",
    "\n",
    "        TOPIC_PATH = BASE_PATH + topic\n",
    "        if not os.path.isdir(TOPIC_PATH):\n",
    "            os.mkdir(TOPIC_PATH)\n",
    "        \n",
    "        link_path_temp = []\n",
    "        small_topic_boxes = driver.find_elements_by_xpath('//div[@class=\"topic-box topic-box-secondary\"]')\n",
    "        for topic_box in small_topic_boxes:\n",
    "            topic_box_heading = topic_box.find_element_by_xpath('.//a[@class=\"topic-box-secondary-heading\"]')\n",
    "\n",
    "            topic_box_name = re.sub(\"\\(see all\\)\", \"\", topic_box_heading.text)\n",
    "            topic_box_name = re.sub(\":\", \"\", topic_box_name)\n",
    "\n",
    "            SMALL_TOPIC_PATH = os.path.join(BASE_PATH + topic, topic_box_name)\n",
    "            if not os.path.isdir(SMALL_TOPIC_PATH):\n",
    "                os.mkdir(SMALL_TOPIC_PATH)\n",
    "\n",
    "            link = topic_box_heading.get_attribute('href')\n",
    "            link_path_temp.append((link, os.path.join(SMALL_TOPIC_PATH, \"all.csv\")))\n",
    "            \n",
    "            topic_box_elements = topic_box.find_elements_by_xpath('.//div[@class=\"l3\"]/a')\n",
    "            for element in topic_box_elements:\n",
    "                link = element.get_attribute(\"href\")\n",
    "                link_path_temp.append((link, os.path.join(SMALL_TOPIC_PATH, element.text + \".csv\")))\n",
    "        \n",
    "        \n",
    "        for link, path in link_path_temp:\n",
    "            vocab_scraping(link, path)\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
